"""Strategy generation using Large Language Models (LLMs).

This module provides the StrategyGenerator class that uses LLMs to generate
quantitative trading strategies. It supports:
- Generating strategy ideas from prompts
- Converting ideas to factor specifications (JSON)
- Converting ideas to executable Python code
- Batch processing of multiple strategies
- Iterative refinement based on validation feedback

The generator uses configurable prompts and can work with different LLM providers
(OpenAI GPT-4, GPT-3.5, Anthropic Claude, etc).

Example:
    >>> generator = StrategyGenerator(model='gpt-4', n_ideas=10)
    >>> ideas = generator.generate_ideas()
    >>> specs = generator.ideas_to_specs_batch(ideas)
    >>> # Compile specs to strategies...
"""

from __future__ import annotations

import json
import logging
import re
import time
from typing import Any

from anthropic import Anthropic
from openai import OpenAI
from pydantic import BaseModel

from alphalab.ai.prompt_loader import (
    PromptLoader,
    load_batch_spec_prompt,
    load_code_generation_prompt,
    load_factor_spec_prompt,
    load_idea_generation_prompt,
)

logger = logging.getLogger(__name__)


# --- Pydantic models for structured output from OpenAI ---


class StrategyIdea(BaseModel):
    """Model for a strategy idea generated by an LLM."""

    name: str
    hypothesis: str
    features: list[str]
    expected_conditions: str
    risks: list[str]


class StrategyIdeas(BaseModel):
    """Collection of strategy ideas."""

    ideas: list[StrategyIdea]


class FactorSpec(BaseModel):
    """Model for a factor specification."""

    family: str
    factors: list[dict[str, Any]]
    combine: dict[str, Any]
    post: list[str]
    rationale: str


class FactorSpecBatch(BaseModel):
    """Collection of factor specifications."""

    specs: list[FactorSpec]


class StrategyGenerator:
    """Generate quantitative trading strategies using LLMs.

    This class uses LLMs to generate strategy ideas and convert them into
    either JSON factor specifications or Python code. It supports:
    - Multiple LLM providers (OpenAI, Anthropic)
    - Batch processing for efficiency
    - Iterative refinement based on feedback
    - Configurable prompts via external markdown files

    Args:
        provider: LLM provider ('openai' or 'anthropic')
        model: Model name (e.g., 'gpt-4', 'claude-3')
        n_ideas: Number of strategy ideas to generate
        temperature: Sampling temperature (0.0-1.0)
        max_retries: Maximum retry attempts on API failures
        use_structured_output: Use structured output (OpenAI only)
        idea_source: Source of ideas ('llm' or 'provided')

    Example:
        >>> generator = StrategyGenerator(model='gpt-4o', n_ideas=5)
        >>> ideas = generator.generate_ideas()
        >>> specs = generator.ideas_to_specs_batch(ideas)
        >>> # Validate and compile specs...
    """

    def __init__(
        self,
        provider: str = "openai",
        model: str = "gpt-4o-mini",
        n_ideas: int = 10,
        temperature: float = 0.7,
        max_retries: int = 3,
        use_structured_output: bool = False,
        idea_source: str = "llm",
        ideas: list | None = None,
        preferences: Any | None = None,
    ):
        """Initialize the strategy generator.

        Args:
            provider: LLM provider ('openai' or 'anthropic')
            model: Model name (e.g., 'gpt-4', 'claude-3')
            n_ideas: Number of strategy ideas to generate
            temperature: Sampling temperature (0.0-1.0)
            max_retries: Maximum retry attempts on API failures
            use_structured_output: Use structured output (OpenAI only)
            idea_source: Source of ideas ('llm' or 'provided')
            ideas: Pre-defined list of strategy ideas (if idea_source='provided')
            preferences: User preferences object for constraining generation
        """
        self.provider = provider.lower()
        self.model = model
        self.n_ideas = n_ideas
        self.temperature = temperature
        self.max_retries = max_retries
        self.use_structured_output = use_structured_output
        self.idea_source = idea_source
        self.provided_ideas = ideas if ideas else []
        self.preferences = preferences

        # Initialize LLM client
        if self.provider == "openai":
            self.client = OpenAI()
        elif self.provider == "anthropic":
            self.client = Anthropic()
            # Claude doesn't support structured output
            self.use_structured_output = False
        else:
            raise ValueError(f"Unsupported provider: {provider}")

        # Initialize prompt loader
        self.prompt_loader = PromptLoader()

        logger.info(
            f"Initialized StrategyGenerator with {provider} {model}, "
            f"n_ideas={n_ideas}, temperature={temperature}"
        )

    def _get_temperature(self) -> float:
        """Get the appropriate temperature for the current model.

        Some models like gpt-5-mini only support the default temperature of 1.0.
        This method returns the correct temperature value based on the model.

        Returns:
            Temperature value to use for API calls
        """
        # Models that only support default temperature
        if self.model in ["gpt-5-mini", "gpt-5"]:
            return 1.0

        # All other models use the configured temperature
        return self.temperature

    def generate_ideas(
        self,
        market_context: str | None = None,
        data_description: dict | None = None,
    ) -> list[dict[str, Any]]:
        """Generate strategy ideas using the LLM.

        Args:
            market_context: Description of current market conditions
            data_description: Available data and features

        Returns:
            List of strategy idea dictionaries

        Raises:
            ValueError: If generation fails after max_retries
        """
        if self.idea_source == "provided":
            logger.info(f"Using {len(self.provided_ideas)} provided ideas")
            return self.provided_ideas

        logger.info(f"Generating {self.n_ideas} strategy ideas...")
        prompt = self._build_idea_prompt(market_context, data_description)

        for attempt in range(self.max_retries):
            try:
                if self.provider == "openai":
                    ideas = self._generate_ideas_openai(prompt)
                else:  # anthropic
                    ideas = self._generate_ideas_anthropic(prompt)

                logger.info(f"Generated {len(ideas)} strategy ideas")
                return ideas

            except Exception as e:
                logger.warning(f"Attempt {attempt + 1} failed: {e}")
                if attempt < self.max_retries - 1:
                    time.sleep(2**attempt)  # Exponential backoff
                else:
                    raise ValueError(f"Failed to generate ideas after {self.max_retries} attempts")

    def _generate_ideas_openai(self, prompt: str) -> list[dict[str, Any]]:
        """Generate ideas using OpenAI API."""
        if self.use_structured_output:
            # Use structured output with Pydantic model
            response = self.client.beta.chat.completions.parse(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are a quantitative trading strategist."},
                    {"role": "user", "content": prompt},
                ],
                temperature=self.temperature,
                response_format=StrategyIdeas,
            )

            ideas_obj = response.choices[0].message.parsed
            return [idea.model_dump() for idea in ideas_obj.ideas]
        else:
            # Use regular completion and parse JSON
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are a quantitative trading strategist."},
                    {"role": "user", "content": prompt},
                ],
                temperature=self._get_temperature(),
            )

            content = response.choices[0].message.content
            # Extract JSON from response
            json_match = re.search(r"\[.*\]", content, re.DOTALL)
            if json_match:
                ideas_json = json_match.group()
                return json.loads(ideas_json)
            else:
                raise ValueError("No valid JSON found in response")

    def _generate_ideas_anthropic(self, prompt: str) -> list[dict[str, Any]]:
        """Generate ideas using Anthropic API."""
        response = self.client.messages.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            temperature=self.temperature,
            max_tokens=4096,
        )

        content = response.content[0].text
        # Extract JSON from response
        json_match = re.search(r"\[.*\]", content, re.DOTALL)
        if json_match:
            ideas_json = json_match.group()
            return json.loads(ideas_json)
        else:
            raise ValueError("No valid JSON found in response")

    def ideas_to_specs_batch(
        self,
        ideas: list[dict[str, Any]],
        compiler=None,
        example_selector=None,
    ) -> list[dict[str, Any]]:
        """Convert multiple ideas to factor specifications in batch.

        This is more efficient than converting ideas one by one, especially
        for large batches. The LLM generates all specifications in a single call.

        Args:
            ideas: List of strategy idea dictionaries
            compiler: FactorSpecCompiler for validation (optional)
            example_selector: FewShotExampleSelector for examples (optional)

        Returns:
            List of factor specification dictionaries

        Raises:
            ValueError: If batch conversion fails
        """
        logger.info(f"Converting {len(ideas)} ideas to specs in batch...")

        # Get available columns from compiler if provided
        available_columns = []
        if compiler:
            available_columns = [
                {"name": col, "description": compiler.get_column_description(col)}
                for col in compiler.available_columns
            ]

        prompt = self._build_batch_spec_prompt(ideas, available_columns, example_selector)

        for attempt in range(self.max_retries):
            try:
                if self.provider == "openai":
                    specs = self._convert_batch_openai(prompt)
                else:  # anthropic
                    specs = self._convert_batch_anthropic(prompt)

                logger.info(f"Generated {len(specs)} factor specifications")
                return specs

            except Exception as e:
                logger.warning(f"Batch conversion attempt {attempt + 1} failed: {e}")
                if attempt < self.max_retries - 1:
                    time.sleep(2**attempt)
                else:
                    raise ValueError(f"Failed after {self.max_retries} attempts")

    def _convert_batch_openai(self, prompt: str) -> list[dict[str, Any]]:
        """Convert batch using OpenAI API."""
        if self.use_structured_output:
            response = self.client.beta.chat.completions.parse(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": "You are an expert in factor-based quantitative trading.",
                    },
                    {"role": "user", "content": prompt},
                ],
                temperature=self.temperature,
                response_format=FactorSpecBatch,
            )

            specs_obj = response.choices[0].message.parsed
            return [spec.model_dump() for spec in specs_obj.specs]
        else:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": "You are an expert in factor-based quantitative trading.",
                    },
                    {"role": "user", "content": prompt},
                ],
                temperature=self.temperature,
            )

            content = response.choices[0].message.content
            # Extract JSON array
            json_match = re.search(r"\[.*\]", content, re.DOTALL)
            if json_match:
                specs_json = json_match.group()
                return json.loads(specs_json)
            else:
                raise ValueError("No valid JSON array found in response")

    def _convert_batch_anthropic(self, prompt: str) -> list[dict[str, Any]]:
        """Convert batch using Anthropic API."""
        response = self.client.messages.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            temperature=self.temperature,
            max_tokens=8192,
        )

        content = response.content[0].text
        # Extract JSON array
        json_match = re.search(r"\[.*\]", content, re.DOTALL)
        if json_match:
            specs_json = json_match.group()
            return json.loads(specs_json)
        else:
            raise ValueError("No valid JSON array found in response")

    def idea_to_spec(
        self,
        idea: dict[str, Any],
        data_schema: dict | None = None,
        validation_errors: list[str] | None = None,
        compiler=None,
    ) -> dict[str, Any]:
        """Convert a single idea to a factor specification.

        Args:
            idea: Strategy idea dictionary
            data_schema: Available data schema
            validation_errors: Previous validation errors for refinement
            compiler: FactorSpecCompiler for getting available columns

        Returns:
            Factor specification dictionary

        Raises:
            ValueError: If conversion fails after max_retries
        """
        logger.info(f"Converting idea '{idea['name']}' to factor spec...")

        prompt = self._build_spec_prompt(idea, data_schema, validation_errors, compiler)

        for attempt in range(self.max_retries):
            try:
                if self.provider == "openai":
                    spec = self._convert_spec_openai(prompt)
                else:  # anthropic
                    spec = self._convert_spec_anthropic(prompt)

                logger.info(f"Generated spec for '{idea['name']}'")
                return spec

            except Exception as e:
                logger.warning(f"Attempt {attempt + 1} failed: {e}")
                if attempt < self.max_retries - 1:
                    time.sleep(2**attempt)
                else:
                    raise ValueError(f"Failed after {self.max_retries} attempts")

    def _convert_spec_openai(self, prompt: str) -> dict[str, Any]:
        """Convert single spec using OpenAI API."""
        if self.use_structured_output:
            response = self.client.beta.chat.completions.parse(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": "You are an expert in factor-based quantitative trading.",
                    },
                    {"role": "user", "content": prompt},
                ],
                temperature=self.temperature,
                response_format=FactorSpec,
            )

            return response.choices[0].message.parsed.model_dump()
        else:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": "You are an expert in factor-based quantitative trading.",
                    },
                    {"role": "user", "content": prompt},
                ],
                temperature=self.temperature,
            )

            content = response.choices[0].message.content
            # Extract JSON object
            json_match = re.search(r"\{.*\}", content, re.DOTALL)
            if json_match:
                spec_json = json_match.group()
                return json.loads(spec_json)
            else:
                raise ValueError("No valid JSON object found in response")

    def _convert_spec_anthropic(self, prompt: str) -> dict[str, Any]:
        """Convert single spec using Anthropic API."""
        response = self.client.messages.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            temperature=self.temperature,
            max_tokens=4096,
        )

        content = response.content[0].text
        # Extract JSON object
        json_match = re.search(r"\{.*\}", content, re.DOTALL)
        if json_match:
            spec_json = json_match.group()
            return json.loads(spec_json)
        else:
            raise ValueError("No valid JSON object found in response")

    def generate_code(self, idea: dict[str, Any], data_schema: dict | None = None) -> str:
        """Convert an idea directly to Python code.

        This method uses the recommended spec compiler approach:
        idea → JSON spec → FactorSpecCompiler → code

        Args:
            idea: Strategy idea dictionary
            data_schema: Available data schema

        Returns:
            Python code string for the strategy

        Raises:
            ValueError: If code generation fails
        """
        # Use the recommended spec compiler approach
        spec = self.idea_to_spec(idea, data_schema)

        # Store the spec for later retrieval
        self.last_spec = spec

        # Compile spec to code
        from alphalab.ai.spec_compiler import FactorSpecCompiler
        compiler = FactorSpecCompiler()
        code = compiler.compile_to_code(spec)

        return code

    def get_last_spec(self) -> dict | None:
        """Get the last generated spec from the previous generation call.

        Returns:
            The last generated spec dictionary, or None if no spec was generated
        """
        return getattr(self, 'last_spec', None)

    def idea_to_code(self, idea: dict[str, Any], data_schema: dict | None = None) -> str:
        """Convert an idea directly to Python code (deprecated).

        This method is deprecated in favor of idea_to_spec + FactorSpecCompiler.
        The two-stage approach (JSON spec -> code) is more reliable and maintainable.

        Args:
            idea: Strategy idea dictionary
            data_schema: Available data schema

        Returns:
            Python code string for the strategy

        Raises:
            ValueError: If code generation fails
        """
        logger.warning("idea_to_code is deprecated. Use idea_to_spec + FactorSpecCompiler instead")

        prompt = self._build_code_prompt(idea, data_schema)

        for attempt in range(self.max_retries):
            try:
                if self.provider == "openai":
                    code = self._generate_code_openai(prompt)
                else:  # anthropic
                    code = self._generate_code_anthropic(prompt)

                logger.info(f"Generated code for '{idea['name']}'")
                return code

            except Exception as e:
                logger.warning(f"Attempt {attempt + 1} failed: {e}")
                if attempt < self.max_retries - 1:
                    time.sleep(2**attempt)
                else:
                    raise ValueError(f"Failed after {self.max_retries} attempts")

    def _generate_code_openai(self, prompt: str) -> str:
        """Generate code using OpenAI API."""
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": "You are an expert Python programmer."},
                {"role": "user", "content": prompt},
            ],
            temperature=self.temperature,
        )

        content = response.choices[0].message.content
        # Extract code from markdown code blocks
        code_match = re.search(r"```python\n(.*?)```", content, re.DOTALL)
        if code_match:
            return code_match.group(1)
        else:
            # Try to find any code-like content
            return content

    def _generate_code_anthropic(self, prompt: str) -> str:
        """Generate code using Anthropic API."""
        response = self.client.messages.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            temperature=self.temperature,
            max_tokens=4096,
        )

        content = response.content[0].text
        # Extract code from markdown code blocks
        code_match = re.search(r"```python\n(.*?)```", content, re.DOTALL)
        if code_match:
            return code_match.group(1)
        else:
            return content

    def _to_class_name(self, name: str) -> str:
        """Convert a strategy name to a valid Python class name."""
        # Remove non-alphanumeric characters and convert to CamelCase
        words = re.sub(r"[^a-zA-Z0-9\s]", "", name).split()
        return "".join(word.capitalize() for word in words)

    def _build_idea_prompt(
        self, market_context: str | None, data_description: dict | None
    ) -> str:
        """Build prompt for idea generation."""
        return load_idea_generation_prompt(
            self.prompt_loader,
            n_ideas=self.n_ideas,
            market_context=market_context,
            available_data=data_description,
            preferences=self.preferences,
        )

    def _build_spec_prompt(
        self,
        idea: dict[str, Any],
        data_schema: dict | None,
        validation_errors: list[str] | None,
        compiler,
    ) -> str:
        """Build prompt for spec generation."""
        return load_factor_spec_prompt(
            self.prompt_loader,
            idea=idea,
            data_schema=data_schema,
            validation_errors=validation_errors,
            compiler=compiler,
        )

    def _build_batch_spec_prompt(
        self,
        ideas: list[dict[str, Any]],
        columns_data: list,
        example_selector,
    ) -> str:
        """Build prompt for batch spec generation."""
        return load_batch_spec_prompt(
            self.prompt_loader,
            ideas=ideas,
            columns_data=columns_data,
            example_selector=example_selector,
        )

    def _build_code_prompt(self, idea: dict[str, Any], data_schema: dict | None) -> str:
        """Build prompt for code generation (deprecated)."""
        return load_code_generation_prompt(
            self.prompt_loader,
            idea=idea,
            data_schema=data_schema,
        )